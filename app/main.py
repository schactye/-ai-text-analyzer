from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import logging
from datetime import datetime
from contextlib import asynccontextmanager

from app.analyzer import TextAnalyzer
from app.models import AnalysisRequest, AnalysisResponse

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
analyzer = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global analyzer
    # Startup
    analyzer = TextAnalyzer()
    logger.info("üöÄ AI Text Analyzer –∑–∞–ø—É—â–µ–Ω!")
    logger.info("üìù –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É: /docs")
    yield
    # Shutdown
    logger.info("üõë AI Text Analyzer –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="AI Text Analyzer",
    description="–ú–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/", tags=["Root"])
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ API"""
    return {
        "message": "üß† AI Text Analyzer API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "analyze_endpoint": "/analyze"
    }

@app.get("/health", tags=["Health"])
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "AI Text Analyzer",
        "version": "1.0.0"
    }

@app.post("/analyze", response_model=AnalysisResponse, tags=["Analysis"])
async def analyze_text(request: AnalysisRequest):
    """
    –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP
    
    - **text**: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π)
    - **include_keywords**: –í–∫–ª—é—á–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: true)
    - **max_keywords**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)
    """
    global analyzer
    
    try:
        if not request.text or len(request.text.strip()) == 0:
            raise HTTPException(status_code=400, detail="–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        if len(request.text) > 10000:
            raise HTTPException(status_code=400, detail="–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–º–∞–∫—Å. 10000 —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        result = analyzer.analyze(
            text=request.text,
            include_keywords=request.include_keywords,
            max_keywords=request.max_keywords
        )
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(request.text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")

@app.post("/batch-analyze", tags=["Analysis"])
async def batch_analyze(texts: List[str], max_keywords: int = 5):
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    global analyzer
    
    try:
        if len(texts) > 100:
            raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 100 —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞ —Ä–∞–∑")
        
        results = []
        for i, text in enumerate(texts):
            if len(text.strip()) > 0:
                result = analyzer.analyze(text, max_keywords=max_keywords)
                result.text_id = i
                results.append(result)
        
        return {"results": results, "processed_count": len(results)}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")

@app.get("/stats", tags=["Statistics"])
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API"""
    global analyzer
    return analyzer.get_stats()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
