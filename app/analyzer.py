import time
import re
from collections import Counter
from datetime import datetime
from typing import List, Dict, Optional
import logging

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è NLP
try:
    from textblob import TextBlob
except ImportError:
    TextBlob = None

try:
    from langdetect import detect, detect_langs
    from langdetect.lang_detect_exception import LangDetectException
except ImportError:
    detect = None

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
except ImportError:
    SentimentIntensityAnalyzer = None

from app.models import (
    AnalysisResponse, SentimentResult, LanguageResult, 
    TextStatistics, KeywordResult, APIStats
)

logger = logging.getLogger(__name__)

class TextAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.start_time = time.time()
        self.stats = {
            'total_requests': 0,
            'total_texts_analyzed': 0,
            'processing_times': [],
            'languages': Counter(),
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
        self.vader_analyzer = SentimentIntensityAnalyzer() if SentimentIntensityAnalyzer else None
        
        # –°–ª–æ–≤–∞—Ä—å —è–∑—ã–∫–æ–≤
        self.language_names = {
            'ru': 'Russian', 'en': 'English', 'es': 'Spanish', 'fr': 'French',
            'de': 'German', 'it': 'Italian', 'pt': 'Portuguese', 'nl': 'Dutch',
            'pl': 'Polish', 'cs': 'Czech', 'sk': 'Slovak', 'uk': 'Ukrainian',
            'bg': 'Bulgarian', 'hr': 'Croatian', 'sl': 'Slovenian', 'et': 'Estonian',
            'lv': 'Latvian', 'lt': 'Lithuanian', 'hu': 'Hungarian', 'ro': 'Romanian',
            'fi': 'Finnish', 'sv': 'Swedish', 'da': 'Danish', 'no': 'Norwegian'
        }
        
        logger.info("üß† TextAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze(self, text: str, include_keywords: bool = True, max_keywords: int = 10) -> 
AnalysisResponse:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        start_time = time.time()
        
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats['total_requests'] += 1
            self.stats['total_texts_analyzed'] += 1
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            sentiment = self._analyze_sentiment(text)
            language = self._detect_language(text)
            statistics = self._calculate_statistics(text)
            keywords = self._extract_keywords(text, max_keywords) if include_keywords else None
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —è–∑—ã–∫–æ–≤
            self.stats['languages'][language.language] += 1
            
            processing_time = (time.time() - start_time) * 1000
            self.stats['processing_times'].append(processing_time)
            
            return AnalysisResponse(
                sentiment=sentiment,
                language=language,
                statistics=statistics,
                keywords=keywords,
                processing_time_ms=processing_time,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            raise
    
    def _analyze_sentiment(self, text: str) -> SentimentResult:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º TextBlob –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
            if TextBlob:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                subjectivity = blob.sentiment.subjectivity
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É
                if polarity > 0.1:
                    label = "positive"
                elif polarity < -0.1:
                    label = "negative"
                else:
                    label = "neutral"
                
                confidence = abs(polarity) if abs(polarity) > 0.1 else 0.5
                
            else:
                # Fallback: –ø—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                positive_words = ['—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '—Å—É–ø–µ—Ä', 
'–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ']
                negative_words = ['–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–∫–æ—à–º–∞—Ä', '—É–∂–∞—Å', '–ø–ª–æ—Ö–æ–π']
                
                text_lower = text.lower()
                pos_count = sum(1 for word in positive_words if word in text_lower)
                neg_count = sum(1 for word in negative_words if word in text_lower)
                
                if pos_count > neg_count:
                    label, polarity, confidence = "positive", 0.6, 0.7
                elif neg_count > pos_count:
                    label, polarity, confidence = "negative", -0.6, 0.7
                else:
                    label, polarity, confidence = "neutral", 0.0, 0.5
                
                subjectivity = 0.5
            
            return SentimentResult(
                label=label,
                confidence=min(confidence, 1.0),
                polarity=max(-1.0, min(1.0, polarity)),
                subjectivity=max(0.0, min(1.0, subjectivity))
            )
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return SentimentResult(
                label="neutral",
                confidence=0.5,
                polarity=0.0,
                subjectivity=0.5
            )
    
    def _detect_language(self, text: str) -> LanguageResult:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        try:
            if detect and len(text.strip()) > 10:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º langdetect
                detected_lang = detect(text)
                langs = detect_langs(text)
                confidence = max([lang.prob for lang in langs if lang.lang == detected_lang])
                
                language_name = self.language_names.get(detected_lang, detected_lang.capitalize())
                
                return LanguageResult(
                    language=detected_lang,
                    language_name=language_name,
                    confidence=confidence
                )
            else:
                # Fallback: –ø—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                if re.search(r'[–∞-—è—ë]', text.lower()):
                    return LanguageResult(language="ru", language_name="Russian", confidence=0.8)
                else:
                    return LanguageResult(language="en", language_name="English", confidence=0.6)
                    
        except (LangDetectException, Exception) as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞: {e}")
            return LanguageResult(language="unknown", language_name="Unknown", confidence=0.1)
    
    def _calculate_statistics(self, text: str) -> TextStatistics:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ –ø–æ–¥—Å—á–µ—Ç—ã
            char_count = len(text)
            words = re.findall(r'\b\w+\b', text)
            word_count = len(words)
            sentences = re.split(r'[.!?]+', text)
            sentence_count = len([s for s in sentences if s.strip()])
            paragraphs = text.split('\n\n')
            paragraph_count = len([p for p in paragraphs if p.strip()])
            
            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0
            avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
            
            # –ü—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
            if word_count > 0 and sentence_count > 0:
                readability = 100 - (avg_word_length * 5 + avg_sentence_length * 2)
                readability = max(0, min(100, readability))
            else:
                readability = 50
            
            return TextStatistics(
                character_count=char_count,
                word_count=word_count,
                sentence_count=sentence_count,
                paragraph_count=max(1, paragraph_count),
                avg_word_length=round(avg_word_length, 2),
                avg_sentence_length=round(avg_sentence_length, 2),
                readability_score=round(readability, 1)
            )
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return TextStatistics(
                character_count=len(text),
                word_count=0,
                sentence_count=1,
                paragraph_count=1,
                avg_word_length=0,
                avg_sentence_length=0,
                readability_score=50
            )
    
    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[KeywordResult]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å—Ç–æ–ø-—Å–ª–æ–≤
            stop_words = {
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', 
'–æ–Ω–∞', '—Ç–∞–∫',
                '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', 
'–º–Ω–µ', '–±—ã–ª–æ',
                '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', 
'–≤–¥—Ä—É–≥',
                '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', 
'–æ–ø—è—Ç—å', '—É–∂',
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 
'are', 'was',
                'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 
'could', 'should'
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
            words = re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', text.lower())
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ —Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
            word_freq = Counter([word for word in words if word not in stop_words])
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            keywords = []
            total_words = sum(word_freq.values())
            
            for word, freq in word_freq.most_common(max_keywords):
                importance = freq / total_words if total_words > 0 else 0
                keywords.append(KeywordResult(
                    word=word,
                    frequency=freq,
                    importance=round(importance, 3)
                ))
            
            return keywords
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
            return []
    
    def get_stats(self) -> APIStats:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API"""
        try:
            avg_time = sum(self.stats['processing_times']) / len(self.stats['processing_times']) if 
self.stats['processing_times'] else 0
            most_common_lang = self.stats['languages'].most_common(1)[0][0] if self.stats['languages'] 
else 'unknown'
            uptime = time.time() - self.start_time
            
            return APIStats(
                total_requests=self.stats['total_requests'],
                total_texts_analyzed=self.stats['total_texts_analyzed'],
                avg_processing_time_ms=round(avg_time, 2),
                most_common_language=most_common_lang,
                uptime_seconds=round(uptime, 2)
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return APIStats(
                total_requests=0,
                total_texts_analyzed=0,
                avg_processing_time_ms=0,
                most_common_language='unknown',
                uptime_seconds=0
            )
